<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>EveryoneIsTheProductManager_Note_V1</title>
    <url>/2024/03/25/EveryoneIsTheProductManager-Note-V1/</url>
    <content><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p><strong>书名：</strong>《人人都是产品经理》<br><strong>阅读日期：</strong>2024&#x2F;03&#x2F;02-2024&#x2F;03&#x2F;12<br><strong>阅读次数：</strong> 1</p>
<hr>
<h2 id="第一次阅读笔记"><a href="#第一次阅读笔记" class="headerlink" title="第一次阅读笔记"></a>第一次阅读笔记</h2><hr>
<span id="more"></span>

<h3 id="Chap-One-–-需求"><a href="#Chap-One-–-需求" class="headerlink" title="Chap_One – 需求"></a>Chap_One – 需求</h3><p>从《营销管理》等一系列书籍与商业案例中，都可得知用户需求对于一个产品的必要性</p>
<img src="/images/Note/IMG_4249.jpg" title="Chap_One_Img_1">
<img src="/images/Note/IMG_4250.jpg" title="Chap_One_Img_2">

<h3 id="Chap-Two-–-项目管理"><a href="#Chap-Two-–-项目管理" class="headerlink" title="Chap_Two – 项目管理"></a>Chap_Two – 项目管理</h3><p>产品是有一个个的大大小小的项目构成，项目管理能力是一个优秀的产品经理必备的能力</p>
<img src="/images/Note/IMG_4251.jpg" title="Chap_Two_Img_1">
<img src="/images/Note/IMG_4252.jpg" title="Chap_Two_Img_2">
<img src="/images/Note/IMG_4253.jpg" title="Chap_Two_Img_3">

<h3 id="Chap-Four-–-战略与领导力"><a href="#Chap-Four-–-战略与领导力" class="headerlink" title="Chap_Four – 战略与领导力"></a>Chap_Four – 战略与领导力</h3><p>产品团队是一个产品得以发展延续的本源动力，产品是关乎所有相关人员的事情</p>
<img src="/images/Note/IMG_4254.jpg" title="Chap_Four_Img_1">
<img src="/images/Note/IMG_4255.jpg" title="Chap_Four_Img_2">

<h3 id="Chap-Five-–-价值观"><a href="#Chap-Five-–-价值观" class="headerlink" title="Chap_Five – 价值观"></a>Chap_Five – 价值观</h3><p>一个有战斗力的团队，是离不开一个统一的思想与目标的指导</p>
<img src="/images/Note/IMG_4256.jpg" title="Chap_Five_Img_1">
<img src="/images/Note/IMG_4257.jpg" title="Chap_Five_Img_2">

<h3 id="Chap-Six-–-PM的素养"><a href="#Chap-Six-–-PM的素养" class="headerlink" title="Chap_Six – PM的素养"></a>Chap_Six – PM的素养</h3><p>PM对应的素养、才能，能够帮助PM发挥相应的战斗力</p>
<img src="/images/Note/IMG_4258.jpg" title="Chap_Six_Img_1">
<img src="/images/Note/IMG_4259.jpg" title="Chap_Six_Img_2">

<h3 id="不足点"><a href="#不足点" class="headerlink" title="不足点"></a>不足点</h3><img src="/images/Note/IMG_4260.jpg" title="Spport">]]></content>
      <categories>
        <category>LearningNote</category>
        <category>PM</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>IE_Race</title>
    <url>/2024/03/26/IE-Race/</url>
    <content><![CDATA[<h2 id="项目原理"><a href="#项目原理" class="headerlink" title="项目原理"></a>项目原理</h2><pre><code>基于融合学习，进行游戏公司玩家流失量预测
</code></pre>
<span id="more"></span>
<h3 id="技术方案计划书"><a href="#技术方案计划书" class="headerlink" title="技术方案计划书"></a>技术方案计划书</h3><iframe src="/images/Project/IE-Race/基于投票的集成学习模型_10-8.pdf">]]></content>
      <tags>
        <tag>项目实践</tag>
      </tags>
  </entry>
  <entry>
    <title>ReinforceLearning_Note_V1</title>
    <url>/2024/03/25/ReinforceLearning-Note-V1/</url>
    <content><![CDATA[<h2 id="DeepReinforceLearning"><a href="#DeepReinforceLearning" class="headerlink" title="DeepReinforceLearning"></a>DeepReinforceLearning</h2><p>主要介绍深度强化学习中的DQn、梯度策略与AlphaGo及AlphaGo Zero</p>
<span id="more"></span>

<h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN:"></a>DQN:</h3><pre><code>Use neural network 𝑄(𝑠,𝑎;𝐰) to approximate 𝑄⋆(𝑠,𝑎).to select the best/most valuable actiton in this state

Q-function to subscribe the value of action in the real world.

Temporal Difference (TD) Learning to make the action evaluation function Q become more accurate.

    cause	𝑈t = 𝑅t + 𝛾⋅𝑈t+1 .
    prediction 𝑄(𝑠t,𝑎t;𝐰t)
    loss	Lt = 1/2*[𝑄(𝑠t,𝑎t;𝐰t) - yt]^2
    Gradient descent wt+1 = wt - alpha*(dLt / dw)|w=wt

    DQN : off-policy like the &quot;behavior cloning&quot; thinking
</code></pre>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient:"></a>Policy Gradient:</h3><pre><code>Neural network to approximate Value and Policy, make the value evaluation function more accurancy and the policy preform better.

    𝑉(𝑠) =∑𝑎 𝜋(𝑎,𝑠)⋅𝑄𝜋(𝑠,𝑎) . contain: actor &amp; critic
    
    𝜋(𝑎,𝑠;𝛉) -&gt; 𝜋(𝑎,𝑠); 𝑞(𝑠,𝑎;𝐰) -&gt; 𝑞(𝑠,𝑎)
    
    𝑉(𝑠;𝛉,𝐰) = ∑𝑎 𝜋(𝑎,𝑠;𝛉)𝑞(𝑠,𝑎;𝐰)

    𝐠(𝑎,𝛉) = [dlog𝜋(𝑎,𝑠;𝛉)/d𝛉]•𝑞(𝑠,𝑎;𝐰)

    d𝑉(𝑠;𝛉,𝐰)/d𝛉 =𝔼𝐴[𝐠(𝐴,𝛉)] &#123;cause 𝜋 is probability density function&#125;

    as for the critic use the TD to update 𝐰
</code></pre>
<h3 id="AlphaGo（MCTS）"><a href="#AlphaGo（MCTS）" class="headerlink" title="AlphaGo（MCTS）"></a>AlphaGo（MCTS）</h3><pre><code>For the Monte Carlo Tree Search trained AlphaGO Zero, it has 4 steps:
    1. Selection: The player makes an action 𝑎. (Imaginary action; not actual move.)

    2. Expansion: The opponent makes an action; the state updates. (Also imaginary action; made by the policy network.)

    3. Evaluation: Evaluate the state-value and get score 𝑣. Play the game to the end to receive reward 𝑟. Assign score (𝑣+𝑟)/2

    4. Backup: Use the score (𝑣+𝑟)/2 to action 𝑎. to update action-values.
</code></pre>
]]></content>
      <categories>
        <category>LearningNote</category>
        <category>RL</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>CrawlerOnWeibo</title>
    <url>/2024/03/25/crawler-form-weibo/</url>
    <content><![CDATA[<p><strong>项目背景：</strong>出于需求微博部分大V提问信息、问题价值、问题回复以及对应回答人员的信息的爬取用于科研，不涉及反反爬内容，仅用最基本功能等。<br><strong>Based on:</strong> Python 3.8</p>
<hr>
<span id="more"></span>

<h2 id="Quite-Start"><a href="#Quite-Start" class="headerlink" title="Quite Start"></a>Quite Start</h2><hr>
<h3 id="Page-One-Get"><a href="#Page-One-Get" class="headerlink" title="Page One Get"></a>Page One Get</h3><pre><code>从EXCEL中读取对应大V的问答链接，进行其所有问答链接的爬取
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">from openpyxl import Workbook</span><br><span class="line">from openpyxl import load_workbook</span><br><span class="line"># import pymysql</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">#请求头</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36&#x27;,</span><br><span class="line">    &#x27;Referer&#x27;: &#x27;https://m.weibo.cn/&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 定义函数获取多页数据，两个参数分别是下一页面的since_id和爬取的博主ID</span><br><span class="line">def get_page(since_id,bigV_id):</span><br><span class="line">    #url变化的部分固定为传入的两个参数的字符串格式str()</span><br><span class="line">    url = &#x27;https://m.weibo.cn/api/container/getIndex?containerid=231068_-_QUESTIONLIST&amp;extparam=&#x27; + str(bigV_id) + &#x27;&amp;since_id=&#x27; + str(since_id)</span><br><span class="line">    try:</span><br><span class="line">        #获取页面的请求许可</span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        #如果请求返回状态码为200表示许可</span><br><span class="line">        if response.status_code == 200:</span><br><span class="line">            #以json文件解析请求数据包</span><br><span class="line">            json = response.json()</span><br><span class="line">            if json[&#x27;ok&#x27;] == 1:</span><br><span class="line">                #如果成功则获取json文件中的cardlistInfo下的since_id</span><br><span class="line">                next_since_id = json.get(&#x27;data&#x27;).get(&#x27;cardlistInfo&#x27;)[&#x27;since_id&#x27;]</span><br><span class="line">                #返回下一页面的since</span><br><span class="line">                return (json, next_since_id)</span><br><span class="line">            else:</span><br><span class="line">                return None</span><br><span class="line">    #下面是出错退出返回</span><br><span class="line">    except requests.ConnectionError as e:</span><br><span class="line">        print(&#x27;Error&#x27;, e.args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#此代码定义函数run()来获取需要的数据，传入参数为需要爬取的微博用户</span><br><span class="line">def run(bigV_id):</span><br><span class="line">    # 创建一个新的Excel工作簿，设置列名称，和工作表名</span><br><span class="line">    newWb = Workbook()</span><br><span class="line">    newWs = newWb.create_sheet(&#x27;1&#x27;, 0)</span><br><span class="line">    # 添加表头</span><br><span class="line">    newWs.append([&#x27;uid&#x27;, &#x27;asker_id&#x27;, &#x27;name_ask&#x27;, &#x27;avatar&#x27;, &#x27;onlookers&#x27;, &#x27;brief_ques&#x27;, &#x27;value&#x27;, &#x27;detailed_url&#x27;])</span><br><span class="line">    #设置一个值表示当前页面，初始为0</span><br><span class="line">    i = 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # while True:</span><br><span class="line">    while True:</span><br><span class="line">        if i == 0: # 未滚动，当前为初始页面</span><br><span class="line">            #创建元组tuple_since_id存当前初始页面的数据</span><br><span class="line">            tuple_since_id = get_page(&#x27;&#x27;,bigV_id)</span><br><span class="line">            # 页面加一</span><br><span class="line">            i+=1</span><br><span class="line">        #如果已经不是初始页面，开始数据搜集</span><br><span class="line">        else:</span><br><span class="line"></span><br><span class="line">            tuple_since_id = get_page(tuple_since_id[1],bigV_id)</span><br><span class="line"></span><br><span class="line">            if tuple_since_id: # 如果获取元组数据</span><br><span class="line">                print(tuple_since_id[1])</span><br><span class="line"></span><br><span class="line">                # 检查新的 since_id 是否为空，如果是，说明只有一个页面，只获取一次数据用break结束</span><br><span class="line">                if tuple_since_id[1] ==&#x27;&#x27;:#这里是单页面的条件，since_id=&#x27;&#x27;</span><br><span class="line">                    #获取元组内的数据解析json</span><br><span class="line">                    json = tuple_since_id[0]</span><br><span class="line">                    # 解析JSON数据提取所需信息，我们需要的事data,card中的部分数据</span><br><span class="line">                    cards = json[&#x27;data&#x27;][&#x27;cards&#x27;]</span><br><span class="line">                    #遍历card中的数据，挨个获取</span><br><span class="line">                    for card in cards:</span><br><span class="line">                        #要的事card中的cardgroup的数据</span><br><span class="line">                        if &#x27;card_group&#x27; in card:</span><br><span class="line">                            row = []  # 建立一个价row的列表，用于存储合并后的一行数据</span><br><span class="line">                            # 遍历cardgroup中的数据，挨个获取</span><br><span class="line">                            for item in card[&#x27;card_group&#x27;]:</span><br><span class="line">                                #需要card_type==93下的数据</span><br><span class="line">                                if item[&#x27;card_type&#x27;] == 93:</span><br><span class="line">                                    #获取user的id</span><br><span class="line">                                    user = item[&#x27;user&#x27;]</span><br><span class="line">                                    if user:#如果获取到</span><br><span class="line">                                        #列表中的数据对应连续添加到Excel的列中，split是必要的内容切分只获取需要的</span><br><span class="line">                                        row.extend([bigV_id, user[&#x27;id&#x27;], user[&#x27;screen_name&#x27;], user[&#x27;profile_image_url&#x27;],</span><br><span class="line">                                                    item[&#x27;status&#x27;].split(&#x27;&lt;/font&gt;&lt;font&#x27;)[0].split(&#x27;&gt;&#x27;)[-1]])</span><br><span class="line">                                    else:#如果没有就置空</span><br><span class="line">                                        row.extend([&#x27;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;&#x27;])</span><br><span class="line">                                #下面同理也是获取card的数据</span><br><span class="line">                                elif item[&#x27;card_type&#x27;] == 8:</span><br><span class="line">                                    row.extend([item[&#x27;title_sub&#x27;], item[&#x27;desc1&#x27;], item[&#x27;scheme&#x27;]])</span><br><span class="line">                                    print(item[&#x27;title_sub&#x27;])</span><br><span class="line">                            newWs.append(row)#增加新的一行</span><br><span class="line">                    print(&quot;一页的数据获取完毕，结束&quot;)</span><br><span class="line">                    #单页面的数据获取完毕，结束</span><br><span class="line">                    break</span><br><span class="line"></span><br><span class="line">                #接下来就是获取需要滚动加载的多页面包的数据，逻辑一致,只是没有结束条件</span><br><span class="line">                else:</span><br><span class="line"></span><br><span class="line">                    json = tuple_since_id[0]</span><br><span class="line">                    # 解析JSON数据提取所需信息</span><br><span class="line">                    cards = json[&#x27;data&#x27;][&#x27;cards&#x27;]</span><br><span class="line">                    for card in cards:</span><br><span class="line">                        if &#x27;card_group&#x27; in card:</span><br><span class="line">                            row = []  # 用于存储合并后的一行数据</span><br><span class="line">                            for item in card[&#x27;card_group&#x27;]:</span><br><span class="line">                                if item[&#x27;card_type&#x27;] == 93:</span><br><span class="line">                                    user = item[&#x27;user&#x27;]</span><br><span class="line">                                    if user:</span><br><span class="line">                                        row.extend([bigV_id,user[&#x27;id&#x27;], user[&#x27;screen_name&#x27;], user[&#x27;profile_image_url&#x27;],item[&#x27;status&#x27;].split(&#x27;&lt;/font&gt;&lt;font&#x27;)[0].split(&#x27;&gt;&#x27;)[-1]])</span><br><span class="line">                                    else:</span><br><span class="line">                                        row.extend([&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;])</span><br><span class="line">                                elif item[&#x27;card_type&#x27;] == 8:</span><br><span class="line">                                    row.extend([item[&#x27;title_sub&#x27;], item[&#x27;desc1&#x27;], item[&#x27;scheme&#x27;]])</span><br><span class="line">                                    print(item[&#x27;title_sub&#x27;])</span><br><span class="line">                            newWs.append(row)</span><br><span class="line"></span><br><span class="line">            else:</span><br><span class="line">                break</span><br><span class="line">        # 滚动到底跳出循环</span><br><span class="line"></span><br><span class="line">    #Excel表格保存save()函数，命名为用户名.xlsx</span><br><span class="line">    newWb.save(str(bigV_id) + &#x27;.xlsx&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#main函数，说明这个工作表的路径和要处理的开始行</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    file_path = r&quot;./merged_file.xlsx&quot;</span><br><span class="line"></span><br><span class="line">    # 读取Excel文件的第2行的第一列数据</span><br><span class="line">    df = pd.read_excel(file_path, usecols=[0], header=None)</span><br><span class="line">    ids = df.iloc[305:350, 0].tolist()</span><br><span class="line">    print(ids)</span><br><span class="line">    #调用run()函数</span><br><span class="line">    # run(1990466285)</span><br><span class="line">    #</span><br><span class="line"></span><br><span class="line">    # #调用run()函数</span><br><span class="line">    for id in ids:</span><br><span class="line">        run(id)</span><br></pre></td></tr></table></figure>

<h3 id="Page-Two-Get"><a href="#Page-Two-Get" class="headerlink" title="Page Two Get"></a>Page Two Get</h3><pre><code>从上述代码获得问答详细链接后进行下一步问答详细信息提取
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用import导入requests模块</span><br><span class="line">import time</span><br><span class="line">import re</span><br><span class="line">import pandas as pd</span><br><span class="line">import requests</span><br><span class="line">from openpyxl import load_workbook</span><br><span class="line"># 从bs4中导入BeautifulSoup</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def run(name: str):</span><br><span class="line">    # 打开Excel文件</span><br><span class="line">    # 通过 load_workbook 函数打开指定的 Excel 文件，并获取活动工作表 ws</span><br><span class="line">    # 注意每次处理的时候修改工作表名字，比如当前的工作表名为“test_1.xlsx”</span><br><span class="line">    # excel_name = &#x27;test_1.xlsx&#x27;</span><br><span class="line"></span><br><span class="line">    count = 0</span><br><span class="line"></span><br><span class="line">    excel_name = name</span><br><span class="line"></span><br><span class="line">    wb = load_workbook(excel_name)</span><br><span class="line">    ws = wb.active</span><br><span class="line"></span><br><span class="line">    # 遍历Excel里面的所有工作表</span><br><span class="line">    for ws in wb.worksheets:</span><br><span class="line">        print(&quot;当前工作表名称：&quot;, ws.title)</span><br><span class="line">        # 获取工作表的最大列数</span><br><span class="line">        max_column = ws.max_column</span><br><span class="line">        # 设置表头，添加三列</span><br><span class="line">        ws.cell(row=1, column=9, value=&#x27;time_ask&#x27;)</span><br><span class="line">        ws.cell(row=1, column=10, value=&#x27;time_ans&#x27;)</span><br><span class="line">        ws.cell(row=1, column=11, value=&#x27;ques&#x27;)</span><br><span class="line"></span><br><span class="line">        # 用函数enumerate获取列索引（当前列），和需要读取的列，规定从第二行开始</span><br><span class="line">        for col_index, col in enumerate(ws.iter_cols(min_col=8, max_col=8, min_row=2, values_only=True), start=2):</span><br><span class="line">            # 每一行的当前行数，获取page_id，从第二行开始</span><br><span class="line">            for row_index, page_id in enumerate(col, start=2):</span><br><span class="line">                # page_id为二级页面需要的url</span><br><span class="line">                if page_id == &#x27; &#x27;:</span><br><span class="line">                    break</span><br><span class="line">                print(&quot;元素所在行：&quot;, row_index)</span><br><span class="line"></span><br><span class="line">                # 将表格中的page_id传给url</span><br><span class="line">                url = f&#x27;&#123;page_id&#125;&#x27;</span><br><span class="line"></span><br><span class="line">                # 请求头</span><br><span class="line">                headers = &#123;</span><br><span class="line">                    &quot;User-Agent&quot;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">                    # &#x27;Referer&#x27;: &#x27;https://media.weibo.cn/api/wenda?&#x27;,</span><br><span class="line">                    # 补充你的cookie</span><br><span class="line">                    &quot;Cookie&quot;: &quot;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">                # 获取反馈</span><br><span class="line">                response = requests.get(url, headers=headers)</span><br><span class="line">                # 解码，防止中文乱码</span><br><span class="line">                response.encoding = response.apparent_encoding</span><br><span class="line"></span><br><span class="line">                if response.status_code == 200:</span><br><span class="line"></span><br><span class="line">                    # 将服务器响应内容转换为字符串形式，赋值给html</span><br><span class="line">                    html = response.text</span><br><span class="line">                    # print(html)</span><br><span class="line">                    # 使用BeautifulSoup()传入变量html和解析器lxml，赋值给soup</span><br><span class="line">                    soup = BeautifulSoup(html, &quot;lxml&quot;)</span><br><span class="line">                    # 使用find_all()查询soup中class=&quot; &quot;的节点</span><br><span class="line"></span><br><span class="line">                    # 获取title结构下的内容，即为需要的详细评论</span><br><span class="line">                    content_all = soup.find_all(&quot;title&quot;)</span><br><span class="line"></span><br><span class="line">                    for content in content_all:</span><br><span class="line">                        # 获取内容</span><br><span class="line">                        text = content.text</span><br><span class="line">                        if &#x27;抱歉，出错了&#x27; in text:</span><br><span class="line">                            text = &#x27;博主设置无法查看&#x27;</span><br><span class="line">                            count += 1</span><br><span class="line">                        print(text)</span><br><span class="line">                        # 你要插入的行号，在一开始我们获取到的row_index</span><br><span class="line">                        row_number = row_index</span><br><span class="line">                        # 将提问的时间填写到对应行的第11列单元格中,根据表格调整</span><br><span class="line">                        ws.cell(row=row_number, column=11, value=text)</span><br><span class="line">                        # 保存工作表</span><br><span class="line">                        wb.save(excel_name)</span><br><span class="line"></span><br><span class="line">                    # 获取提问时间和回答时间，是标记在DIV中间，CLASS_名为“text S_txt2&quot;的内容</span><br><span class="line">                    div_all = soup.find_all(&quot;div&quot;, class_=&quot;text S_txt2&quot;)</span><br><span class="line">                    # 建一个列表来存</span><br><span class="line">                    time_all = []</span><br><span class="line">                    # 遍历存，连续地写到列表中</span><br><span class="line">                    for div in div_all:</span><br><span class="line">                        text = div.get_text(strip=True)  # 使用strip=True去除文本前后的空白字符</span><br><span class="line">                        time_all.append(text)</span><br><span class="line"></span><br><span class="line">                    # 使用正则表达式提取时间部分，符合该格式的才会被提取出来，可以看到&#123;&#125;内的使我们提取的信息</span><br><span class="line">                    pattern_Q = r&#x27;(\d&#123;2&#125;-\d&#123;2&#125; \d&#123;2&#125;:\d&#123;2&#125; )提问&#x27;</span><br><span class="line">                    pattern_A = r&#x27;(\d&#123;2&#125;-\d&#123;2&#125; \d&#123;2&#125;:\d&#123;2&#125; )回答&#x27;</span><br><span class="line"></span><br><span class="line">                    # 然后再使用正则表达式提取时间部分</span><br><span class="line">                    for time_text in time_all:</span><br><span class="line">                        match_A = re.search(pattern_A, time_text)</span><br><span class="line">                        match_Q = re.search(pattern_Q, time_text)</span><br><span class="line"></span><br><span class="line">                        if match_A:</span><br><span class="line">                            # 获取answer回答的时间</span><br><span class="line">                            time_answer = match_A.group(1)</span><br><span class="line">                            print(time_answer)</span><br><span class="line">                            # 你要插入的行号</span><br><span class="line">                            row_number = row_index</span><br><span class="line">                            # row_number = 2</span><br><span class="line">                            # 将回答的时间填写到对应行的第10列单元格中</span><br><span class="line">                            ws.cell(row=row_number, column=10, value=time_answer)</span><br><span class="line">                            # 保存工作表</span><br><span class="line">                            wb.save(excel_name)</span><br><span class="line"></span><br><span class="line">                        if match_Q:</span><br><span class="line">                            # 获取question提问的时间</span><br><span class="line">                            time_ask = match_Q.group(1)</span><br><span class="line">                            print(time_ask)</span><br><span class="line"></span><br><span class="line">                            # 假设你要插入的行号是2（示例）</span><br><span class="line">                            row_number = row_index</span><br><span class="line">                            # row_number = 2</span><br><span class="line"></span><br><span class="line">                            # 将提问的时间填写到对应行的第9列单元格中</span><br><span class="line">                            ws.cell(row=row_number, column=9, value=time_ask)</span><br><span class="line">                            # 保存工作表</span><br><span class="line">                            wb.save(excel_name)</span><br><span class="line"></span><br><span class="line">                    # time.sleep(1)</span><br><span class="line"></span><br><span class="line">    if count &gt;= 5:</span><br><span class="line">        wb.save(&quot;./many/&quot; + excel_name)</span><br><span class="line">    # 保存工作表</span><br><span class="line">    else:</span><br><span class="line">        wb.save(&quot;./less/&quot; + excel_name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    file_path = r&quot;./merged_file.xlsx&quot;</span><br><span class="line">    # 读取Excel文件的第2行的第一列数据</span><br><span class="line">    df = pd.read_excel(file_path, usecols=[0], header=None)</span><br><span class="line">    ids = df.iloc[292:350, 0].tolist()</span><br><span class="line">    print(ids, &quot;\n&quot;)</span><br><span class="line">    # 调用run()函数</span><br><span class="line">    for id in ids:</span><br><span class="line">        try:</span><br><span class="line">            print(id, type(id))</span><br><span class="line">            # run(str(id) + &#x27;.xlsx&#x27;)</span><br><span class="line">            name = &quot;&#123;&#125;&quot;.format(id) + &quot;.xlsx&quot;</span><br><span class="line">            print(name, type(name), &quot;\n&quot;)</span><br><span class="line">            run(name=name)</span><br><span class="line"></span><br><span class="line">        except:</span><br><span class="line">            continue</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="End-Note"><a href="#End-Note" class="headerlink" title="End Note"></a>End Note</h3><p>仅仅如此，简单的一个小作坊手搓easy代码完成，能跑就行。略显潦草，后续有兴趣可以进行下一步的数据清洗、存储数据库、进行反反爬等。</p>
]]></content>
      <categories>
        <category>Project</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>项目实践</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2024/02/22/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
</search>
